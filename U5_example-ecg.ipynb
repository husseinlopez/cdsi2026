{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# U5: Ejemplo de detección de estrés con señales ECG\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/husseinlopez/cdsi2026/blob/main/U5_example-ecg.ipynb)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#!pip install heartpy scipy scikit-learn pandas numpy matplotlib seaborn"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# system\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# data\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# signal processing\nimport heartpy as hp\nfrom scipy.signal import welch\nfrom scipy.interpolate import interp1d\n\n# machine learning\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import LeaveOneGroupOut\nfrom sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### WESAD dataset"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "WESAD (Wearable Stress and Affect Detection) es un dataset multimodal para la detección de estrés y afecto. Contiene señales ECG de 15 sujetos recolectadas con un dispositivo RespiBAN Professional a 700 Hz.\n\nLas etiquetas de condición son:\n- **1**: baseline (reposo)\n- **2**: stress (tarea de aritmética mental / Trier Social Stress Test)\n- **3**: amusement (clips de video divertidos)\n\nDataset en Kaggle (requiere cuenta): https://www.kaggle.com/datasets/qiriro/wesad\n\nPara descargarlo directamente desde Colab:\n1. En Kaggle → Account → Create New Token → descarga `kaggle.json`\n2. Sube el archivo cuando la celda siguiente lo solicite"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom google.colab import files\n\n# upload kaggle.json when prompted\nos.makedirs('/root/.kaggle', exist_ok=True)\nuploaded = files.upload()  # select kaggle.json here\n\nwith open('/root/.kaggle/kaggle.json', 'wb') as f:\n    f.write(uploaded['kaggle.json'])\nos.chmod('/root/.kaggle/kaggle.json', 0o600)\n\n# download and unzip (~2 GB, may take a few minutes)\n!pip install -q kaggle\n!kaggle datasets download -d qiriro/wesad -p /content --unzip\n\nWESAD_PATH = '/content/WESAD'\n\n# subjects available\nsubjects = ['S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'S10', 'S11',\n            'S13', 'S14', 'S15', 'S16', 'S17']\n\nSAMPLE_RATE = 700  # Hz, chest ECG"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Loading a single subject"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pickle\n\ndef load_subject(subject_id):\n    path = os.path.join(WESAD_PATH, subject_id, f'{subject_id}.pkl')\n    with open(path, 'rb') as f:\n        data = pickle.load(f, encoding='latin1')\n    \n    ecg  = data['signal']['chest']['ECG'].flatten()\n    label = data['label']\n    \n    return ecg, label\n\necg_raw, labels = load_subject('S2')\n\nprint(f'ECG samples: {len(ecg_raw)}')\nprint(f'Duration: {len(ecg_raw) / SAMPLE_RATE / 60:.1f} min')\nprint(f'Label values: {np.unique(labels)}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exploratory analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# show a 10-second segment per condition\nconditions = {1: 'baseline', 2: 'stress', 3: 'amusement'}\ncolors     = {1: 'steelblue', 2: 'tomato', 3: 'mediumseagreen'}\n\nfig, axes = plt.subplots(3, 1, figsize=(14, 6), sharex=True)\n\nfor ax, (cond, name) in zip(axes, conditions.items()):\n    # find first occurrence of this condition\n    idx = np.where(labels == cond)[0][0]\n    segment = ecg_raw[idx : idx + 10 * SAMPLE_RATE]\n    ax.plot(segment, color=colors[cond], linewidth=0.8)\n    ax.set_ylabel(name, fontsize=11)\n    ax.set_xlim(0, len(segment))\n    sns.despine(ax=ax)\n\naxes[-1].set_xlabel('samples')\nplt.suptitle('ECG raw signal — S2', fontsize=13)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# label distribution\nlabel_series = pd.Series(labels)\ncounts = label_series[label_series.isin([1, 2, 3])].map(conditions).value_counts()\n\nplt.figure(figsize=(6, 3))\ncounts.plot(kind='bar', color=['steelblue', 'tomato', 'mediumseagreen'])\nplt.ylabel('# samples (700 Hz)')\nplt.xlabel('condition')\nplt.xticks(rotation=0)\nsns.despine()\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Preprocessing"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Before extracting features, the raw ECG needs to be filtered and the R-peaks detected. We use HeartPy for this — it handles both bandpass filtering and adaptive peak detection."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# bandpass filter: keep cardiac frequencies (0.5 – 40 Hz)\necg_filtered = hp.filter_signal(ecg_raw,\n                                cutoff=[0.5, 40],\n                                sample_rate=SAMPLE_RATE,\n                                filtertype='bandpass')\n\n# show effect of filter on a 5-second window\nt = np.arange(5 * SAMPLE_RATE)\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 5), sharex=True)\nax1.plot(t, ecg_raw[1000:1000 + 5 * SAMPLE_RATE], linewidth=0.8, color='gray')\nax1.set_title('raw', fontsize=11)\nax2.plot(t, ecg_filtered[1000:1000 + 5 * SAMPLE_RATE], linewidth=0.8, color='steelblue')\nax2.set_title('filtered (bandpass 0.5–40 Hz)', fontsize=11)\nfor ax in [ax1, ax2]:\n    ax.set_ylabel('mV')\n    sns.despine(ax=ax)\nax2.set_xlabel('samples')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# run peak detection on a short segment to visualize\nsegment = ecg_filtered[0 : 20 * SAMPLE_RATE]\n\nwd, m = hp.process(hp.scale_data(segment), sample_rate=SAMPLE_RATE)\n\nplt.figure(figsize=(14, 4))\nhp.plotter(wd, m, title='R-peak detection — S2 (20 s)')\nplt.show()\n\nprint('\\nMeasures from this segment:')\nfor k, v in m.items():\n    print(f'  {k}: {v:.4f}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Windowing"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "A continuous ECG recording cannot be fed directly into a classifier. Instead, we divide the signal into fixed-length windows and extract one feature vector per window. \n\nTwo parameters control this:\n- **window size**: how many seconds of signal to use — long enough to compute HRV reliably (at least 30 s), short enough to capture changes over time\n- **overlap**: how much consecutive windows share — here we use 50% overlap to increase the number of samples without losing temporal resolution"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "WINDOW_SEC  = 60     # window length in seconds\nOVERLAP     = 0.5    # 50% overlap\nWINDOW_SAMP = WINDOW_SEC * SAMPLE_RATE\nSTEP_SAMP   = int(WINDOW_SAMP * (1 - OVERLAP))\n\ndef get_windows(ecg, labels, sample_rate=SAMPLE_RATE,\n                window_samp=WINDOW_SAMP, step_samp=STEP_SAMP,\n                valid_labels=(1, 2)):\n    \"\"\"Slide a window over the ECG and return segments with their majority label.\"\"\"\n    windows, window_labels = [], []\n    for start in range(0, len(ecg) - window_samp, step_samp):\n        end = start + window_samp\n        seg_label = labels[start:end]\n        # keep only baseline and stress windows with consistent labeling\n        unique, counts = np.unique(seg_label, return_counts=True)\n        majority = unique[np.argmax(counts)]\n        purity   = counts.max() / len(seg_label)\n        if majority in valid_labels and purity >= 0.9:\n            windows.append(ecg[start:end])\n            window_labels.append(majority)\n    return windows, window_labels\n\n# test on S2\nwindows, window_labels = get_windows(ecg_filtered, labels)\nprint(f'Total windows: {len(windows)}')\nprint(f'Label distribution: {pd.Series(window_labels).map({1: \"baseline\", 2: \"stress\"}).value_counts().to_dict()}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# visualize windowing scheme on the label timeline\nfig, ax = plt.subplots(figsize=(14, 2.5))\n\ntime_axis = np.arange(len(labels)) / SAMPLE_RATE / 60\nlabel_mapped = pd.Series(labels).map({1: 1, 2: 2, 3: 3}).fillna(0)\n\nax.fill_between(time_axis, label_mapped, alpha=0.4, step='pre',\n                color='gray', label='condition')\nax.set_yticks([1, 2, 3])\nax.set_yticklabels(['baseline', 'stress', 'amusement'])\nax.set_xlabel('time (min)')\nax.set_title('Label timeline — S2')\nsns.despine(ax=ax)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Feature extraction"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We extract features from three domains, matching what was covered in the slides."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Time-domain features"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Time-domain HRV metrics are computed directly from the RR intervals (distances between consecutive R-peaks)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def extract_time_domain(rr_intervals):\n    \"\"\"\n    rr_intervals: array of RR intervals in milliseconds\n    Returns a dict with SDNN, RMSSD, pNN50, mean HR.\n    \"\"\"\n    if len(rr_intervals) < 5:\n        return None\n    \n    rr   = np.array(rr_intervals)\n    diff = np.diff(rr)\n    \n    features = {\n        'mean_rr'  : np.mean(rr),\n        'sdnn'     : np.std(rr),\n        'rmssd'    : np.sqrt(np.mean(diff**2)),\n        'pnn50'    : np.sum(np.abs(diff) > 50) / len(diff) * 100,\n        'mean_hr'  : 60000 / np.mean(rr),\n    }\n    return features"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Frequency-domain features"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The RR interval series is interpolated to a uniform grid and then the Power Spectral Density (PSD) is estimated with Welch's method. We integrate the PSD in three bands:\n- **VLF**: 0.003 – 0.04 Hz  \n- **LF**: 0.04 – 0.15 Hz (sympathetic + parasympathetic activity)\n- **HF**: 0.15 – 0.4 Hz (parasympathetic / respiratory sinus arrhythmia)  \n- **LF/HF ratio**: classic autonomic balance index"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def extract_frequency_domain(rr_intervals, fs_interp=4.0):\n    \"\"\"\n    Interpolates the RR series to uniform sampling and computes PSD bands.\n    fs_interp: resampling frequency in Hz (4 Hz is standard for HRV).\n    \"\"\"\n    if len(rr_intervals) < 10:\n        return None\n    \n    rr = np.array(rr_intervals)\n    \n    # cumulative time axis (in seconds)\n    t_rr = np.cumsum(rr) / 1000.0\n    t_rr -= t_rr[0]\n    \n    # interpolate to uniform grid\n    t_uniform = np.arange(t_rr[0], t_rr[-1], 1.0 / fs_interp)\n    if len(t_uniform) < 16:\n        return None\n    \n    interp_fn = interp1d(t_rr, rr, kind='cubic', fill_value='extrapolate')\n    rr_uniform = interp_fn(t_uniform)\n    \n    # Welch PSD\n    freqs, psd = welch(rr_uniform, fs=fs_interp, nperseg=min(256, len(rr_uniform)))\n    \n    def band_power(f_low, f_high):\n        idx = (freqs >= f_low) & (freqs < f_high)\n        return np.trapz(psd[idx], freqs[idx])\n    \n    vlf = band_power(0.003, 0.04)\n    lf  = band_power(0.04,  0.15)\n    hf  = band_power(0.15,  0.40)\n    total = vlf + lf + hf\n    \n    features = {\n        'lf_power'   : lf,\n        'hf_power'   : hf,\n        'lf_hf_ratio': lf / hf if hf > 0 else 0,\n        'lf_norm'    : lf / (total - vlf) * 100 if (total - vlf) > 0 else 0,\n        'hf_norm'    : hf / (total - vlf) * 100 if (total - vlf) > 0 else 0,\n    }\n    return features"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Non-linear features (Poincaré plot)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The Poincaré plot maps each RR interval against the next one (RR_n vs RR_{n+1}). SD1 captures short-term variability (parasympathetic), SD2 captures long-term variability."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def extract_nonlinear(rr_intervals):\n    \"\"\"SD1, SD2 and SD2/SD1 from Poincaré plot.\"\"\"\n    if len(rr_intervals) < 5:\n        return None\n    \n    rr   = np.array(rr_intervals)\n    rr1  = rr[:-1]\n    rr2  = rr[1:]\n    \n    sd1 = np.std((rr2 - rr1) / np.sqrt(2))\n    sd2 = np.std((rr2 + rr1) / np.sqrt(2))\n    \n    features = {\n        'sd1'      : sd1,\n        'sd2'      : sd2,\n        'sd2_sd1'  : sd2 / sd1 if sd1 > 0 else 0,\n    }\n    return features"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# visualize Poincare plot for baseline vs stress\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\nfor ax, (cond, name) in zip(axes, {1: 'baseline', 2: 'stress'}.items()):\n    # grab first valid window of this condition\n    idx = [i for i, l in enumerate(window_labels) if l == cond][0]\n    seg = windows[idx]\n    try:\n        wd, m = hp.process(hp.scale_data(seg), sample_rate=SAMPLE_RATE)\n        rr = np.array(wd['RR_list_cor'])\n        ax.scatter(rr[:-1], rr[1:], alpha=0.5, s=15, color=colors[cond])\n        ax.set_title(f'Poincaré — {name}', fontsize=11)\n        ax.set_xlabel('RR_n (ms)')\n        ax.set_ylabel('RR_{n+1} (ms)')\n        ax.set_aspect('equal')\n        sns.despine(ax=ax)\n    except:\n        ax.set_title(f'{name} — processing error')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Building the feature matrix"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def extract_all_features(ecg_segment, sample_rate=SAMPLE_RATE):\n    \"\"\"Run HeartPy on one window and extract all three feature domains.\"\"\"\n    try:\n        wd, m = hp.process(hp.scale_data(ecg_segment),\n                           sample_rate=sample_rate,\n                           clean_rr=True)\n        rr = np.array(wd['RR_list_cor'])\n        if len(rr) < 10:\n            return None\n        \n        td = extract_time_domain(rr)\n        fd = extract_frequency_domain(rr)\n        nl = extract_nonlinear(rr)\n        \n        if td is None or fd is None or nl is None:\n            return None\n        \n        return {**td, **fd, **nl}\n    except:\n        return None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# extract features for all windows of S2\nrecords = []\nfor win, lbl in zip(windows, window_labels):\n    feats = extract_all_features(win)\n    if feats is not None:\n        feats['label']   = lbl\n        feats['subject'] = 'S2'\n        records.append(feats)\n\ndf_s2 = pd.DataFrame(records)\nprint(df_s2.shape)\ndf_s2.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# compare features between conditions\nfeature_cols = [c for c in df_s2.columns if c not in ['label', 'subject']]\nlabel_names  = {1: 'baseline', 2: 'stress'}\ndf_s2['condition'] = df_s2['label'].map(label_names)\n\nfig, axes = plt.subplots(3, 4, figsize=(16, 9))\naxes = axes.flatten()\n\nfor ax, feat in zip(axes, feature_cols):\n    sns.boxplot(data=df_s2, x='condition', y=feat,\n                palette={'baseline': 'steelblue', 'stress': 'tomato'},\n                ax=ax, width=0.5)\n    ax.set_title(feat, fontsize=10)\n    ax.set_xlabel('')\n    sns.despine(ax=ax)\n\nfor ax in axes[len(feature_cols):]:\n    ax.set_visible(False)\n\nplt.suptitle('Feature distributions — S2', fontsize=13)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Multi-subject feature matrix"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We repeat the same pipeline for all subjects to build a dataset suitable for Leave-One-Subject-Out evaluation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "all_records = []\n\nfor subj in subjects:\n    try:\n        ecg, lbl = load_subject(subj)\n        ecg_f = hp.filter_signal(ecg, cutoff=[0.5, 40],\n                                  sample_rate=SAMPLE_RATE,\n                                  filtertype='bandpass')\n        wins, win_lbls = get_windows(ecg_f, lbl)\n        \n        n_ok = 0\n        for win, wlbl in zip(wins, win_lbls):\n            feats = extract_all_features(win)\n            if feats is not None:\n                feats['label']   = wlbl\n                feats['subject'] = subj\n                all_records.append(feats)\n                n_ok += 1\n        \n        print(f'{subj}: {n_ok} windows')\n    except Exception as e:\n        print(f'{subj}: skipped ({e})')\n\ndf_all = pd.DataFrame(all_records)\ndf_all['condition'] = df_all['label'].map({1: 'baseline', 2: 'stress'})\nprint(f'\\nTotal: {len(df_all)} windows — {df_all[\"subject\"].nunique()} subjects')\ndf_all.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# class balance per subject\npivot = df_all.groupby(['subject', 'condition']).size().unstack(fill_value=0)\npivot.plot(kind='bar', figsize=(12, 4),\n           color=['steelblue', 'tomato'],\n           width=0.7)\nplt.ylabel('# windows')\nplt.xlabel('subject')\nplt.xticks(rotation=45)\nplt.legend(title='condition')\nsns.despine()\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Classification"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Leave-One-Subject-Out (LOSO) evaluation"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "In previous units we split data randomly into train/test. For physiological data this is problematic: the same subject appearing in both sets inflates accuracy because the model learns individual physiology rather than generalizable patterns.\n\nLeave-One-Subject-Out (LOSO) is a stricter protocol: on each fold, one subject is held out completely as the test set, and the remaining subjects are used for training. This simulates deploying the model on a new, unseen person."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "feature_cols = [c for c in df_all.columns if c not in ['label', 'subject', 'condition']]\n\nX = df_all[feature_cols].values\ny = df_all['label'].values\ngroups = df_all['subject'].values\n\nlogo = LeaveOneGroupOut()\n\nclassifiers = {\n    'Decision Tree' : DecisionTreeClassifier(random_state=42),\n    'kNN'           : KNeighborsClassifier(n_neighbors=5),\n    'Random Forest' : RandomForestClassifier(n_estimators=100, random_state=42),\n}\n\nresults = {name: {'y_true': [], 'y_pred': []} for name in classifiers}\n\nfor train_idx, test_idx in logo.split(X, y, groups):\n    X_train, X_test = X[train_idx], X[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n    \n    scaler  = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test  = scaler.transform(X_test)\n    \n    for name, clf in classifiers.items():\n        clf.fit(X_train, y_train)\n        pred = clf.predict(X_test)\n        results[name]['y_true'].extend(y_test)\n        results[name]['y_pred'].extend(pred)\n\n# print scores\nfor name in classifiers:\n    y_true = results[name]['y_true']\n    y_pred = results[name]['y_pred']\n    f1 = f1_score(y_true, y_pred, average='macro')\n    acc = np.mean(np.array(y_true) == np.array(y_pred))\n    print(f'{name:20s}  accuracy: {acc:.3f}   F1 (macro): {f1:.3f}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Confusion matrices"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\nclass_names = ['baseline', 'stress']\n\nfor ax, name in zip(axes, classifiers):\n    cm = confusion_matrix(results[name]['y_true'],\n                          results[name]['y_pred'],\n                          labels=[1, 2])\n    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n    \n    disp = ConfusionMatrixDisplay(confusion_matrix=cm_norm,\n                                   display_labels=class_names)\n    disp.plot(ax=ax, colorbar=False, cmap='Blues')\n    ax.set_title(name, fontsize=11)\n\nplt.suptitle('Confusion matrices — LOSO (% per true class)', fontsize=12)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Feature importance (Random Forest)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "rf = classifiers['Random Forest']\n\n# refit on all data to get stable importances\nscaler_full = StandardScaler()\nX_scaled = scaler_full.fit_transform(X)\nrf.fit(X_scaled, y)\n\nimportances = pd.Series(rf.feature_importances_, index=feature_cols)\nimportances = importances.sort_values(ascending=True)\n\nplt.figure(figsize=(8, 5))\nimportances.plot(kind='barh', color='steelblue')\nplt.xlabel('importance')\nplt.title('Feature importance — Random Forest')\nsns.despine()\nplt.tight_layout()\nplt.show()"
  }
 ]
}